{
  "tabGeneral": "Allgemein",
  "tabLlm": "LLM Backend",
  "tabTemplates": "Vorlagen",

  "headerGeneral": "Allgemeine Einstellungen",
  "audioPort": "Audio-Server Port",
  "audioPortHelp": "HTTP-Port für den Audio-Empfang (ESP32, Browser etc.)",
  "whisperModel": "Whisper Modell",
  "whisperModelHelp": "Größe des Whisper-Modells. Größere Modelle sind genauer, brauchen aber mehr RAM und Zeit.",
  "whisperLanguage": "Sprache",
  "whisperLanguageHelp": "ISO-Sprachcode für die Spracherkennung (z.B. de, en, fr)",

  "headerTts": "Text-to-Speech (TTS)",
  "ttsEnabled": "TTS aktivieren",
  "ttsEnabledHelp": "Antworten als Sprache zurückgeben (für ESP32 oder Browser-Wiedergabe)",
  "ttsBackend": "TTS Backend",
  "ttsBackendHelp": "Piper läuft lokal, Edge-TTS nutzt Microsofts kostenlosen Online-Dienst",
  "piperModel": "Piper Modell",
  "piperModelHelp": "Name des Piper-Modells (z.B. de_DE-thorsten-high)",
  "edgeTtsVoice": "Edge-TTS Stimme",
  "edgeTtsVoiceHelp": "Name der Edge-TTS Stimme (z.B. de-DE-ConradNeural)",

  "testWhisper": "Whisper testen",

  "headerLlm": "LLM Backend Konfiguration",
  "llmBackend": "LLM Backend",
  "llmBackendHelp": "Welches KI-Backend für die Sprachverarbeitung verwenden",
  "ollamaUrl": "Ollama URL",
  "ollamaUrlHelp": "URL des lokalen Ollama-Servers",
  "ollamaModel": "Ollama Modell",
  "ollamaModelHelp": "Name des Ollama-Modells (z.B. phi3:mini, llama3, mistral)",
  "ollamaNoThink": "Denkmodus deaktivieren",
  "ollamaNoThinkHelp": "Bei Modellen mit Reasoning/Thinking (z.B. Qwen3) den Denkmodus abschalten. Sendet /no_think Tag um schnellere und direktere Antworten zu erhalten.",
  "openaiApiKey": "OpenAI API-Key",
  "openaiApiKeyHelp": "Ihr OpenAI API-Schlüssel (beginnt mit sk-...)",
  "openaiModel": "OpenAI Modell",
  "openaiModelHelp": "Name des OpenAI-Modells (z.B. gpt-4o-mini, gpt-4o)",
  "anthropicApiKey": "Anthropic API-Key",
  "anthropicApiKeyHelp": "Ihr Anthropic API-Schlüssel (beginnt mit sk-ant-...)",
  "anthropicModel": "Anthropic Modell",
  "anthropicModelHelp": "Name des Claude-Modells (z.B. claude-sonnet-4-20250514)",
  "maxContextTokens": "Max. Kontext-Tokens",
  "maxContextTokensHelp": "Maximale Anzahl an Tokens für den Kontext (System-Prompt + ioBroker-Zustände)",
  "headerToolCalling": "Tool Calling",
  "toolCallingEnabled": "Tool Calling aktivieren",
  "toolCallingEnabledHelp": "Statt JSON aus der LLM-Antwort zu parsen, werden strukturierte Tool-Aufrufe (Function Calling) verwendet. Zuverlässiger, erfordert aber ein Modell mit Tool-Support (z.B. llama3, gpt-4o, claude).",
  "ollamaToolModel": "Tool-Modell (optional)",
  "ollamaToolModelHelp": "Separates Ollama-Modell nur für Tool Calling (z.B. functiongemma). Wenn leer, wird das Hauptmodell für alles verwendet. Sinnvoll wenn ein kleines spezialisiertes Modell die Funktionsauswahl übernehmen soll.",
  "maxToolRounds": "Max. Tool-Runden",
  "maxToolRoundsHelp": "Maximale Anzahl an Tool-Aufruf-Runden pro Anfrage (Sicherheitslimit)",
  "testLlm": "LLM-Verbindung testen",

  "headerTemplates": "Vorlagen-Konfiguration",
  "templateInfo": "Vorlagen definieren, wie der Assistent auf bestimmte Spracheingaben reagiert. Jede Vorlage hat Trigger-Wörter, einen System-Prompt, Kontext-Quellen und erlaubte Aktionen.",
  "templates": "Vorlagen",
  "templateId": "ID",
  "templateName": "Name",
  "templateDescription": "Beschreibung",
  "templateTriggerWords": "Trigger-Wörter (kommagetrennt)",
  "templateSystemPrompt": "System-Prompt",
  "templateContextSources": "Kontext-Quellen (JSON)",
  "templateAllowedActions": "Erlaubte Aktionen (JSON)",
  "templateResponseFormat": "Format",
  "templateMaxContextStates": "Max. States",

  "contextSourcesHelp": "**Kontext-Quellen** und **Erlaubte Aktionen** werden als JSON-Arrays angegeben.\n\n**Kontext-Quellen:** `[{\"pattern\": \"hm-rpc.0.*.TEMPERATURE\", \"label\": \"Temperaturen\"}]`\nOptional mit `\"role\"` für Filterung nach State-Rolle.\n\n**Erlaubte Aktionen:** `[{\"pattern\": \"hm-rpc.0.*.SET_POINT_TEMPERATURE\", \"label\": \"Sollwert setzen\"}]`\n\nWildcard `*` matcht ein Pfadsegment. Verwenden Sie `{context}` im System-Prompt als Platzhalter für die geladenen Daten.",

  "headerImportExport": "Import / Export",
  "importExportInfo": "Vorlagen als JSON exportieren oder importieren. Beim Import werden bestehende Vorlagen mit gleicher ID überschrieben.",
  "exportTemplates": "Vorlagen exportieren",
  "importTemplates": "Vorlagen importieren",
  "templateJsonField": "JSON Import/Export",
  "templateJsonFieldHelp": "Hier JSON einfügen zum Importieren oder Export-Ergebnis kopieren"
}
