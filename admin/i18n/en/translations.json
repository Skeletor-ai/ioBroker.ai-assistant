{
  "tabGeneral": "General",
  "tabLlm": "LLM Backend",
  "tabTemplates": "Templates",

  "headerGeneral": "General Settings",
  "audioPort": "Audio Server Port",
  "audioPortHelp": "HTTP port for audio reception (ESP32, browser, etc.)",
  "whisperModel": "Whisper Model",
  "whisperModelHelp": "Whisper model size. Larger models are more accurate but require more RAM and time.",
  "whisperLanguage": "Language",
  "whisperLanguageHelp": "ISO language code for speech recognition (e.g. de, en, fr)",

  "headerTts": "Text-to-Speech (TTS)",
  "ttsEnabled": "Enable TTS",
  "ttsEnabledHelp": "Return responses as speech audio (for ESP32 or browser playback)",
  "ttsBackend": "TTS Backend",
  "ttsBackendHelp": "Piper runs locally, Edge-TTS uses Microsoft's free online service",
  "piperModel": "Piper Model",
  "piperModelHelp": "Piper model name (e.g. de_DE-thorsten-high)",
  "edgeTtsVoice": "Edge-TTS Voice",
  "edgeTtsVoiceHelp": "Edge-TTS voice name (e.g. de-DE-ConradNeural)",

  "testWhisper": "Test Whisper",

  "headerLlm": "LLM Backend Configuration",
  "llmBackend": "LLM Backend",
  "llmBackendHelp": "Which AI backend to use for language processing",
  "ollamaUrl": "Ollama URL",
  "ollamaUrlHelp": "URL of the local Ollama server",
  "ollamaModel": "Ollama Model",
  "ollamaModelHelp": "Ollama model name (e.g. phi3:mini, llama3, mistral)",
  "openaiApiKey": "OpenAI API Key",
  "openaiApiKeyHelp": "Your OpenAI API key (starts with sk-...)",
  "openaiModel": "OpenAI Model",
  "openaiModelHelp": "OpenAI model name (e.g. gpt-4o-mini, gpt-4o)",
  "anthropicApiKey": "Anthropic API Key",
  "anthropicApiKeyHelp": "Your Anthropic API key (starts with sk-ant-...)",
  "anthropicModel": "Anthropic Model",
  "anthropicModelHelp": "Claude model name (e.g. claude-sonnet-4-20250514)",
  "maxContextTokens": "Max Context Tokens",
  "maxContextTokensHelp": "Maximum number of tokens for context (system prompt + ioBroker states)",
  "testLlm": "Test LLM Connection",

  "headerTemplates": "Template Configuration",
  "templateInfo": "Templates define how the assistant responds to specific voice inputs. Each template has trigger words, a system prompt, and allowed actions. Context sources and allowed actions are specified as comma-separated ioBroker state patterns (e.g. 'hm-rpc.0.*.TEMPERATURE' for all temperature sensors).",
  "templates": "Templates",
  "templateId": "ID",
  "templateName": "Name",
  "templateTriggerWords": "Trigger Words (comma-separated)",
  "templateSystemPrompt": "System Prompt",
  "templateResponseFormat": "Response Format",
  "templateMaxContextStates": "Max Context States",

  "headerAdvanced": "Advanced Configuration Notes",
  "advancedInfo": "For context sources (contextSources) and allowed actions (allowedActions), you can define sub-tables with 'pattern' and 'label' fields directly in JSON format within the template objects. Example: [{\"pattern\": \"hm-rpc.0.*.TEMPERATURE\", \"label\": \"Temperatures\"}]. These advanced fields can be edited via file configuration or the JSON object."
}
